{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,) int32\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "def load_cifar10(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Fetch the CIFAR-10 dataset from the web and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 dataset and use appropriate data types and shapes\n",
    "    cifar10 = tf.keras.datasets.cifar10.load_data()\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    y_train = np.asarray(y_train, dtype=np.int32).flatten()\n",
    "    X_test = np.asarray(X_test, dtype=np.float32)\n",
    "    y_test = np.asarray(y_test, dtype=np.int32).flatten()\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean pixel and divide by std\n",
    "    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n",
    "    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n",
    "    X_train = (X_train - mean_pixel) / std_pixel\n",
    "    X_val = (X_val - mean_pixel) / std_pixel\n",
    "    X_test = (X_test - mean_pixel) / std_pixel\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# If there are errors with SSL downloading involving self-signed certificates,\n",
    "# it may be that your Python version was recently installed on the current machine.\n",
    "# See: https://github.com/tensorflow/tensorflow/issues/10779\n",
    "# To fix, run the command: /Applications/Python\\ 3.7/Install\\ Certificates.command\n",
    "#   ...replacing paths as necessary.\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "NHW = (0, 1, 2)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct a Dataset object to iterate over data X and labels y\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Numpy array of data, of any shape\n",
    "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))\n",
    "\n",
    "\n",
    "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
    "val_dset = Dataset(X_val, y_val, batch_size=64, shuffle=False)\n",
    "test_dset = Dataset(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part34(model_init_fn, optimizer_init_fn, num_epochs=1, is_training=False):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.keras. It trains\n",
    "    a model for one epoch on the CIFAR-10 training set and periodically checks\n",
    "    accuracy on the CIFAR-10 validation set.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_init_fn: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = model_init_fn()\n",
    "    - optimizer_init_fn: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = optimizer_init_fn()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints progress during training\n",
    "    \"\"\"    \n",
    "    with tf.device(device):\n",
    "\n",
    "        # Compute the loss like we did in Part II\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        \n",
    "        model = model_init_fn()\n",
    "        optimizer = optimizer_init_fn()\n",
    "        \n",
    "        train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "    \n",
    "        val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "        val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
    "        \n",
    "        t = 0\n",
    "\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            # Reset the metrics - https://www.tensorflow.org/alpha/guide/migration_guide#new-style_metrics\n",
    "            train_loss.reset_states()\n",
    "            train_accuracy.reset_states()\n",
    "            \n",
    "            for x_np, y_np in train_dset:\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    \n",
    "                    # Use the model function to build the forward pass.\n",
    "                    scores = model(x_np, training=is_training)\n",
    "                    loss = loss_fn(y_np, scores)\n",
    "      \n",
    "                    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                    \n",
    "                    # Update the metrics\n",
    "                    train_loss.update_state(loss)\n",
    "                    train_accuracy.update_state(y_np, scores)\n",
    "                    \n",
    "                    if t % print_every == 0:\n",
    "                        val_loss.reset_states()\n",
    "                        val_accuracy.reset_states()\n",
    "                        for test_x, test_y in val_dset:\n",
    "                            # During validation at end of epoch, training set to False\n",
    "                            prediction = model(test_x, training=False)\n",
    "                            t_loss = loss_fn(test_y, prediction)\n",
    "\n",
    "                            val_loss.update_state(t_loss)\n",
    "                            val_accuracy.update_state(test_y, prediction)\n",
    "                        \n",
    "                        template = 'Iteration {:6d}, Epoch {:2d}, Loss: {:.4f}, Accuracy: {:.2%}, Val Loss: {:.4f}, Val Accuracy: {:.2%}'\n",
    "                        print (template.format(t, epoch+1,\n",
    "                                             train_loss.result(),\n",
    "                                             #train_accuracy.result()*100,\n",
    "                                             train_accuracy.result(),\n",
    "                                             val_loss.result(),\n",
    "                                             #val_accuracy.result()*100))\n",
    "                                             val_accuracy.result()))\n",
    "                        #print('Epoch: ', epoch+1)\n",
    "                        #print('  Iteration: {}'.format(t))\n",
    "                        #print('  train      loss:      {:.3f}'.format(train_loss.result()))\n",
    "                        #print('             accuracy:  {:.3f}'.format(train_accuracy.result()*100))\n",
    "                        #print('  validation loss:      {:.3f}'.format(val_loss.result()))\n",
    "                        #print('             accuracy:  {:.3f}'.format(val_accuracy.result()*100))\n",
    "\n",
    "                    t += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration      0, Epoch  1, Loss: 3.3116, Accuracy: 15.62%, Val Loss: 3.2062, Val Accuracy: 11.50%\n",
      "Iteration    200, Epoch  1, Loss: 1.7823, Accuracy: 35.18%, Val Loss: 1.6950, Val Accuracy: 40.90%\n",
      "Iteration    400, Epoch  1, Loss: 1.6142, Accuracy: 41.28%, Val Loss: 1.3560, Val Accuracy: 52.30%\n",
      "Iteration    600, Epoch  1, Loss: 1.4961, Accuracy: 45.57%, Val Loss: 1.3468, Val Accuracy: 53.10%\n",
      "Iteration    800, Epoch  2, Loss: 1.0876, Accuracy: 61.21%, Val Loss: 1.2033, Val Accuracy: 57.70%\n",
      "Iteration   1000, Epoch  2, Loss: 1.0528, Accuracy: 62.27%, Val Loss: 1.3313, Val Accuracy: 51.20%\n",
      "Iteration   1200, Epoch  2, Loss: 1.0163, Accuracy: 63.83%, Val Loss: 1.1686, Val Accuracy: 58.60%\n",
      "Iteration   1400, Epoch  2, Loss: 0.9878, Accuracy: 64.88%, Val Loss: 0.9862, Val Accuracy: 64.60%\n",
      "Iteration   1600, Epoch  3, Loss: 0.8472, Accuracy: 69.61%, Val Loss: 1.0631, Val Accuracy: 63.80%\n",
      "Iteration   1800, Epoch  3, Loss: 0.8533, Accuracy: 69.84%, Val Loss: 1.0680, Val Accuracy: 62.50%\n",
      "Iteration   2000, Epoch  3, Loss: 0.8361, Accuracy: 70.54%, Val Loss: 0.9904, Val Accuracy: 66.80%\n",
      "Iteration   2200, Epoch  3, Loss: 0.8262, Accuracy: 70.87%, Val Loss: 0.9526, Val Accuracy: 67.20%\n",
      "Iteration   2400, Epoch  4, Loss: 0.7442, Accuracy: 74.04%, Val Loss: 0.9914, Val Accuracy: 65.50%\n",
      "Iteration   2600, Epoch  4, Loss: 0.7511, Accuracy: 73.76%, Val Loss: 0.8632, Val Accuracy: 70.70%\n",
      "Iteration   2800, Epoch  4, Loss: 0.7380, Accuracy: 74.16%, Val Loss: 0.8718, Val Accuracy: 68.80%\n",
      "Iteration   3000, Epoch  4, Loss: 0.7321, Accuracy: 74.21%, Val Loss: 0.8919, Val Accuracy: 70.40%\n",
      "Iteration   3200, Epoch  5, Loss: 0.6817, Accuracy: 75.82%, Val Loss: 0.8212, Val Accuracy: 71.20%\n",
      "Iteration   3400, Epoch  5, Loss: 0.6842, Accuracy: 75.97%, Val Loss: 0.9087, Val Accuracy: 68.00%\n",
      "Iteration   3600, Epoch  5, Loss: 0.6781, Accuracy: 76.17%, Val Loss: 0.8032, Val Accuracy: 72.80%\n",
      "Iteration   3800, Epoch  5, Loss: 0.6706, Accuracy: 76.40%, Val Loss: 0.8394, Val Accuracy: 70.60%\n"
     ]
    }
   ],
   "source": [
    "class CustomConvNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomConvNet, self).__init__()\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        reg = 0     # 0.0001\n",
    "        input_shape = (32, 32, 3)\n",
    "        channel_1, channel_2, channel_3, channel_4, num_classes = 64, 32, 16, 8, 10\n",
    "        \n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    \n",
    "        self.act = tf.keras.layers.Activation(activation='relu')\n",
    "\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv2D(input_shape = (32, 32, 3),\n",
    "                                            filters = channel_1, kernel_size = 3, strides = 1, padding = 'same',\n",
    "                                            use_bias=True,\n",
    "                                            kernel_initializer = initializer, bias_initializer = tf.zeros_initializer,\n",
    "                                            kernel_regularizer = tf.keras.regularizers.l2(l=reg))\n",
    "        \n",
    "        self.bn = tf.keras.layers.BatchNormalization(axis=-1) # spatial batch norm sur C, tf format NHWC\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization(axis=-1) # spatial batch norm sur C, tf format NHWC\n",
    "        \n",
    "        self.conv1_ = tf.keras.layers.Conv2D(filters = channel_1, kernel_size = 3, strides = 1, padding = 'same',\n",
    "                                             use_bias=True,\n",
    "                                             kernel_initializer = initializer, bias_initializer = tf.zeros_initializer,\n",
    "                                             kernel_regularizer = tf.keras.regularizers.l2(l=reg))\n",
    "        \n",
    "        self.bn1_ = tf.keras.layers.BatchNormalization(axis=-1) # spatial batch norm sur C, tf format NHWC\n",
    "        \n",
    "        #self.ln = tf.keras.layers.LayerNormalization(axis=3)\n",
    "        #self.ln = tf.keras.layers.LayerNormalization(axis=3)\n",
    "        \n",
    "\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters = channel_2, kernel_size = 3, strides = 1, padding = 'same',\n",
    "                                            use_bias=True,\n",
    "                                            kernel_initializer = initializer, bias_initializer = tf.zeros_initializer,\n",
    "                                            kernel_regularizer = tf.keras.regularizers.l2(l=reg))\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization(axis=-1)\n",
    "\n",
    "        self.conv2_ = tf.keras.layers.Conv2D(filters = channel_2, kernel_size = 3, strides = 1, padding = 'same',\n",
    "                                            use_bias=True,\n",
    "                                            kernel_initializer = initializer, bias_initializer = tf.zeros_initializer,\n",
    "                                            kernel_regularizer = tf.keras.regularizers.l2(l=reg))\n",
    "        self.bn2_ = tf.keras.layers.BatchNormalization(axis=-1)\n",
    "        \n",
    "        self.conv3 = tf.keras.layers.Conv2D(filters = channel_3, kernel_size = 3, strides = 1, padding = 'same',\n",
    "                                            use_bias=True,\n",
    "                                            kernel_initializer = initializer, bias_initializer = tf.zeros_initializer,\n",
    "                                            kernel_regularizer = tf.keras.regularizers.l2(l=reg))\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization(axis=-1)\n",
    "        \n",
    "        self.conv3_ = tf.keras.layers.Conv2D(filters = channel_3, kernel_size = 3, strides = 1, padding = 'same',\n",
    "                                            use_bias=True,\n",
    "                                            kernel_initializer = initializer, bias_initializer = tf.zeros_initializer,\n",
    "                                            kernel_regularizer = tf.keras.regularizers.l2(l=reg))\n",
    "        self.bn3_ = tf.keras.layers.BatchNormalization(axis=-1)\n",
    "        \n",
    "        self.conv4 = tf.keras.layers.Conv2D(filters = channel_4, kernel_size = 3, strides = 1, padding = 'same',\n",
    "                                            use_bias=True,\n",
    "                                            kernel_initializer = initializer, bias_initializer = tf.zeros_initializer,\n",
    "                                            kernel_regularizer = tf.keras.regularizers.l2(l=reg))\n",
    "        self.bn4 = tf.keras.layers.BatchNormalization(axis=-1)\n",
    "\n",
    "        self.conv4_ = tf.keras.layers.Conv2D(filters = channel_4, kernel_size = 3, strides = 1, padding = 'same',\n",
    "                                            use_bias=True,\n",
    "                                            kernel_initializer = initializer, bias_initializer = tf.zeros_initializer,\n",
    "                                            kernel_regularizer = tf.keras.regularizers.l2(l=reg))\n",
    "        self.bn4_ = tf.keras.layers.BatchNormalization(axis=-1)\n",
    "\n",
    "        #self.conv4 = tf.keras.layers.Conv2D(filters = channel_4, kernel_size = 3, strides = 1, padding = 'same',\n",
    "        #                                    activation='relu', use_bias=True,\n",
    "        #                                    kernel_initializer = initializer, bias_initializer = tf.zeros_initializer)\n",
    "        \n",
    "        self.pooling = tf.keras.layers.MaxPooling2D()\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "    \n",
    "    \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "    \n",
    "    def call(self, input_tensor, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.bn1(x, training)\n",
    "        #x = self.ln(x, training)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        x = self.conv1_(x)\n",
    "        x = self.bn1_(x, training)\n",
    "        x = self.act(x)        \n",
    "        \n",
    "        x = self.pooling(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        x = self.conv2_(x)\n",
    "        x = self.bn2_(x, training)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        x = self.pooling(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        #x = self.conv3_(x)\n",
    "        #x = self.bn3_(x, training)\n",
    "        #x = self.act(x)\n",
    "        \n",
    "        #x = self.pooling(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x, training)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        #x = self.conv4_(x)\n",
    "        #x = self.bn4_(x, training)\n",
    "        #x = self.act(x)\n",
    "        \n",
    "        x = self.pooling(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "\n",
    "device = '/device:GPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "#device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "\n",
    "print_every = 200\n",
    "num_epochs  = 5\n",
    "\n",
    "#print_every = 100\n",
    "#num_epochs  = 1\n",
    "\n",
    "#print_every = 300\n",
    "#num_epochs  = 10\n",
    "\n",
    "model = CustomConvNet()\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomConvNet()\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 2e-3\n",
    "    #learning_rate = 5e-4\n",
    "    return tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.99,  decay=1e-6) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.99,  decay=1e-6) \n",
    "    Iteration    300, Epoch  1, Loss: 2.0678, Accuracy: 23.60%, Val Loss: 1.7464, Val Accuracy: 35.10%\n",
    "    Iteration   7500, Epoch 10, Loss: 0.8114, Accuracy: 71.25%, Val Loss: 0.9961, Val Accuracy: 66.90%\n",
    "    pooling on every layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
